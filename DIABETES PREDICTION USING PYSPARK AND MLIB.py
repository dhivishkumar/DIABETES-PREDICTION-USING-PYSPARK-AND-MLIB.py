# -*- coding: utf-8 -*-
"""BDA_Mini_Project_Diabetes_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XEKSSGx18C1qn1KEsnFf1jSNy4qisZrK

#Installing Dependencies & Initiating a New Spark Session
"""

#install pyspark
! pip install pyspark

#creating a sparksession
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("diabetes_spark").getOrCreate()

"""#Load Dataset"""

#create a spark dataframe
df = spark.read.csv("/content/PNDM.csv",header=True, inferSchema=True)

#displaying the dataframe
df.show()

print("Shape:", (df.count(), len(df.columns)))

#printing the schema
df.printSchema()

#count the total no. of diabetic and non-diabetic class.
print((df.count(),len(df.columns)))
df.groupBy('PNDM').count().show()

"""#Data Preparation"""

#checking for null values
for col in df.columns:
  print(col+":",df[df[col].isNull()].count())

from pyspark.sql.functions import when
df = df.withColumn("Genetic Info", when(df["Genetic Info"] == "No mutation", 0).otherwise(1))
df = df.withColumn("Family History", when(df["Family History"] == "Yes", 1).otherwise(0))
df = df.withColumn("Developmental Delay", when(df["Developmental Delay"] == "Yes", 0).otherwise(1))

df.show()

"""#Performing Feature Selection"""

#feature selection
from pyspark.ml.feature import VectorAssembler
assembler= VectorAssembler(inputCols=['Age','HbA1c','Genetic Info','Family History','Birth Weight','Developmental Delay','Insulin Level'],outputCol='features')
output_data= assembler.transform(df)
output_data.show()

#print the schema
output_data.printSchema()

"""#Split Dataset & Build the Model"""

#create final data
final_data = output_data.select('features','PNDM')

#print schema of final data
final_data.printSchema()

#split the dataset and build the model
train,test= df.randomSplit([0.7,0.3])

"""# **Without HyperParameter Tuning**

**DecisionTreeClassifier**
"""

from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator

from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Create a VectorAssembler to assemble the feature columns into a single 'features' column

assembler= VectorAssembler(inputCols=['Age','HbA1c','Genetic Info','Family History','Birth Weight','Developmental Delay','Insulin Level'],outputCol='features')

# Create a Decision Tree Classifier
decision_tree = DecisionTreeClassifier(labelCol="PNDM", featuresCol="features", maxDepth=4)

# Define the evaluator
evaluator = BinaryClassificationEvaluator(labelCol="PNDM", rawPredictionCol="rawPrediction", metricName="areaUnderROC")

# Create a pipeline with the stages
dt_pipeline = Pipeline(stages=[assembler, decision_tree])


# Fit the pipeline on the training data
dt_model = dt_pipeline.fit(train)

# Make predictions on the test data
dt_predict = dt_model.transform(test)

# Show the first 5 rows of predictions
dt_predict.select("PNDM", "prediction").show(5)

# Define the evaluator
evaluator = BinaryClassificationEvaluator(labelCol="PNDM", rawPredictionCol="rawPrediction", metricName="areaUnderROC")

# Calculate the accuracy
dt_auc = evaluator.evaluate(dt_predict)
print("Decision Tree AUC =", dt_auc)

"""**RandomForestClassifier**"""

from pyspark.ml.classification import RandomForestClassifier

from pyspark.ml import Pipeline
ran_f = RandomForestClassifier(featuresCol="features", labelCol="PNDM", numTrees=100, maxDepth=4, seed=142)
# Define the pipeline stages
stages = [assembler, ran_f]

# Create a pipeline
rf_pipeline = Pipeline(stages=stages)


# Fit the pipeline on the training data
rf_model = rf_pipeline.fit(train)

# Make predictions on the test data
rf_predictions = rf_model.transform(test)

# Show the first 5 rows of predictions
rf_predictions.select("PNDM", "prediction").show(5)

# Random Forest Evaluation
rf_auc = evaluator.evaluate(rf_predictions)
print("Random Forest AUC =", rf_auc)

"""**LogisticRegression**"""

from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator


# Create a Logistic Regression Classifier
logistic_reg = LogisticRegression(labelCol="PNDM", featuresCol="features")

# Define the evaluator
evaluator = BinaryClassificationEvaluator(labelCol="PNDM", rawPredictionCol="rawPrediction", metricName="areaUnderROC")

# Create a pipeline with the stages
lg_pipeline = Pipeline(stages=[assembler, logistic_reg])

# Fit the pipeline to your data
lg_model = lg_pipeline.fit(train)

# Make predictions
lg_predictions = lg_model.transform(test)

# Evaluate the model
log_auc = evaluator.evaluate(lg_predictions)
print("Logistic Regression AUC = ", log_auc)

"""**GBTClassifier**"""

from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import GBTClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Create a GBT Classifier
gbt_classifier = GBTClassifier(labelCol="PNDM", featuresCol="features")

# Define the evaluator
evaluator = BinaryClassificationEvaluator(labelCol="PNDM", rawPredictionCol="rawPrediction", metricName="areaUnderROC")

# Create a pipeline with the stages
gbt_pipeline = Pipeline(stages=[assembler, gbt_classifier])

# Fit the pipeline to your data
gbt_model = gbt_pipeline.fit(train)

# Make predictions
gbt_predictions = gbt_model.transform(test)

# Evaluate the model
gbt_auc = evaluator.evaluate(gbt_predictions)
print("GBT Classifier AUC:", gbt_auc)

"""**LinearSVC**"""

from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LinearSVC
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Create a LinearSVC classifier
svc_classifier = LinearSVC(labelCol="PNDM", featuresCol="features")

# Define the evaluator
evaluator = BinaryClassificationEvaluator(labelCol="PNDM", rawPredictionCol="rawPrediction", metricName="areaUnderROC")

# Create a pipeline with the stages
svc_pipeline = Pipeline(stages=[assembler, svc_classifier])

# Fit the pipeline to your data
svc_model = svc_pipeline.fit(train)

# Make predictions
svc_predictions = svc_model.transform(test)

# Evaluate the model
svc_auc = evaluator.evaluate(svc_predictions)
print("LinearSVC AUC =", svc_auc)

"""# **With HyperParameter Tuning**"""

# Define feature and target columns
feature_column = "features"
target_column = "PNDM"

"""**DecisionTreeClassifier**"""

from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit

# Decision Tree Classifier
dt = DecisionTreeClassifier(labelCol=target_column, featuresCol=feature_column)

# Create a ParamGrid for hyperparameter tuning
paramGrid = (ParamGridBuilder()
    .addGrid(dt.maxDepth, [5, 10, 15])  # Maximum depth of the tree
    .addGrid(dt.impurity, ['gini', 'entropy'])  # Criterion for information gain
    .build())

# Train-validation split
tvs_dt = TrainValidationSplit(
    estimator=dt,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    trainRatio=0.8
)

# Fit the model with grid search
dt_model_tuned = tvs_dt.fit(train)
best_dt_model = dt_model_tuned.bestModel

# Make predictions and evaluate
best_dt_predictions = best_dt_model.transform(test)
best_dt_auc = evaluator.evaluate(best_dt_predictions)
print("Tuned Decision Tree AUC =", best_dt_auc)

"""**RandomForestClassifier**"""

from pyspark.ml.classification import RandomForestClassifier

# Random Forest Classifier
rf = RandomForestClassifier(labelCol=target_column, featuresCol=feature_column)

# Create a ParamGrid for hyperparameter tuning
paramGrid = (ParamGridBuilder()
    .addGrid(rf.numTrees, [10, 20, 30])
    .addGrid(rf.maxDepth, [5, 10, 15])
    .build())

# Train-validation split
tvs = TrainValidationSplit(
    estimator=rf,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    trainRatio=0.8
)

# Fit the model with grid search and bestModel
rf_model_tuned = tvs.fit(train)
best_rf_model = rf_model_tuned.bestModel

# Make predictions and evaluate
best_rf_predictions = best_rf_model.transform(test)
best_rf_auc = evaluator.evaluate(best_rf_predictions)
print("Tuned Random Forest AUC =", best_rf_auc)

"""**LogisticRegression**"""

from pyspark.ml.classification import LogisticRegression

# Logistic Regression
lr = LogisticRegression(labelCol=target_column, featuresCol=feature_column)

# Create a ParamGrid for hyperparameter tuning
paramGrid = (ParamGridBuilder()
    .addGrid(lr.regParam, [0.01, 0.1, 1.0])
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])
    .build())

# Train-validation split
tvs_lr = TrainValidationSplit(
    estimator=lr,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    trainRatio=0.8
)

# Fit the model with grid search
lr_model_tuned = tvs_lr.fit(train)
best_lr_model = lr_model_tuned.bestModel

# Make predictions and evaluate
best_lr_predictions = best_lr_model.transform(test)
best_lr_auc = evaluator.evaluate(best_lr_predictions)
print("Tuned Logistic Regression AUC =", best_lr_auc)

"""**GBTClassifier**"""

from pyspark.ml.classification import GBTClassifier

# Gradient-Boosted Trees (GBT) Classifier
gbt = GBTClassifier(labelCol=target_column, featuresCol=feature_column)

# Create a ParamGrid for hyperparameter tuning
paramGrid = (ParamGridBuilder()
    .addGrid(gbt.maxDepth, [5, 10, 15])
    .addGrid(gbt.maxIter, [10, 20, 30])
    .build())

# Train-validation split
tvs_gbt = TrainValidationSplit(
    estimator=gbt,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    trainRatio=0.8
)

# Fit the model with grid search
gbt_model_tuned = tvs_gbt.fit(train)
best_gbt_model = gbt_model_tuned.bestModel

# Make predictions and evaluate
best_gbt_predictions = best_gbt_model.transform(test)
best_gbt_auc = evaluator.evaluate(best_gbt_predictions)
print("Tuned GBT Classifier AUC =", best_gbt_auc)

""" **LinearSVC**"""

from pyspark.ml.classification import LinearSVC

# Linear Support Vector Machine (SVM)
svm = LinearSVC(labelCol=target_column, featuresCol=feature_column, maxIter=10)

# Create a ParamGrid for hyperparameter tuning
paramGrid = (ParamGridBuilder()
    .addGrid(svm.regParam, [0.01, 0.1, 1.0])
    .build())

# Train-validation split
tvs_svm = TrainValidationSplit(
    estimator=svm,
    estimatorParamMaps=paramGrid,
    evaluator=evaluator,
    trainRatio=0.8
)

# Fit the model with grid search
svm_model_tuned = tvs_svm.fit(train)
best_svm_model = svm_model_tuned.bestModel

# Make predictions and evaluate
best_svm_predictions = best_svm_model.transform(test)
best_svm_auc = evaluator.evaluate(best_svm_predictions)
print("Tuned SVM AUC =", best_svm_auc)